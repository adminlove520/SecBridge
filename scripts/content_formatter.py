import os
import re

def extract_info_from_path(file_path, repo_root, source_config=None):
    """
    æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œä»“åº“é…ç½®æå–æ ‡é¢˜ã€æ ‡ç­¾å’Œå†…å®¹
    """
    source_config = source_config or {}
    source_type = source_config.get('type', 'generic')
    tag_rules = source_config.get('tag_rules', {})
    
    rel_path = os.path.relpath(file_path, repo_root)
    parts = rel_path.split(os.sep)
    
    final_title = os.path.basename(file_path).replace('.md', '')
    tags = set()
    skip = False

    # ---å†…å®¹è¯»å– ---
    full_text = ""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            full_text = f.read()
    except Exception as e:
        full_text = f"æ— æ³•è¯»å–å†…å®¹: {str(e)}"

    # --- 1. è¿‡æ»¤é€»è¾‘: è¯†åˆ«å¯¼èˆªç±» README ---
    if final_title.lower() == 'readme':
        # å¯å‘å¼åˆ¤æ–­ï¼šé“¾æ¥å¯†åº¦
        links = re.findall(r'\[.*?\]\(.*?\)', full_text)
        text_no_links = re.sub(r'\[.*?\]\(.*?\)', '', full_text)
        # å¦‚æœé“¾æ¥æ•°é‡å¤šä¸”å‰©ä½™æ–‡æœ¬è¾ƒå°‘ï¼Œåˆ™è®¤ä¸ºæ˜¯å¯¼èˆª
        if len(links) > 5 and len(text_no_links.strip()) < 300:
            skip = True

    # --- 2. æ ‡ç­¾ç”Ÿæˆç®—æ³• ---
    
    # A. è·¯å¾„åŒ¹é…é€»è¾‘
    path_mapping = tag_rules.get('path_mapping', {})
    for path_part in parts:
        if path_part in path_mapping:
            tags.add(path_mapping[path_part])
    
    # B. Frontmatter æå– (å¦‚ tag: xxx)
    if tag_rules.get('extract_frontmatter'):
        # å…¼å®¹ --- \n tag: xxx \n ---
        fm_match = re.search(r'^---\s*\n(.*?)\n---\s*\n', full_text, re.DOTALL)
        if fm_match:
            fm_content = fm_match.group(1)
            # ç®€å•æ­£åˆ™åŒ¹é… tag/tags
            tag_val = re.search(r'^tags?:\s*(.*)$', fm_content, re.MULTILINE | re.IGNORECASE)
            if tag_val:
                t_str = tag_val.group(1).strip()
                # å¤„ç† [tag1, tag2] æ ¼å¼æˆ–å•å­—ç¬¦ä¸²
                if t_str.startswith('[') and t_str.endswith(']'):
                    t_list = [t.strip().strip('"').strip("'") for t in t_str[1:-1].split(',')]
                    tags.update(t_list)
                else:
                    tags.add(t_str)

    # C. è‡ªåŠ¨ç›®å½•æ ‡ç­¾ (Redteam é£æ ¼: "1. ä¿¡æ¯æ”¶é›†")
    if tag_rules.get('use_folder_as_tag') and len(parts) >= 2:
        folder_tag = parts[-2]
        # å»æ‰æ•°å­—å‰ç¼€ï¼ˆå¦‚ 1. ï¼‰
        folder_tag = re.sub(r'^\d+[\.\s\-]+', '', folder_tag)
        if folder_tag and folder_tag.lower() not in ['source', 'poc', 'readme']:
            tags.add(folder_tag)

    # D. PoC ç‰¹å®šè§„åˆ™
    if source_type == 'poc':
        if len(parts) >= 1 and re.match(r'^\d{4}$', parts[0]):
            tags.add(parts[0])
        vuln_dir_name = parts[-2] if len(parts) >= 2 else ""
        if vuln_dir_name and final_title.lower() in ['readme', 'index', 'poc']:
            final_title = vuln_dir_name
        year = parts[0] if len(parts) >= 1 and parts[0].isdigit() else ""
        if year and not final_title.startswith(year):
            final_title = f"{year}-{final_title}"

    # E. æ­£åˆ™æå– (å¦‚ CVE)
    if tag_rules.get('extract_cve'):
        cve_match = re.search(r'CVE-\d{4}-\d{4,}', final_title, re.IGNORECASE)
        if cve_match:
            tags.add("CVE")

    # --- 3. å†…å®¹æå–ä¸æ¸…æ´— ---
    content_body = ""
    # ç§»é™¤ YAML Frontmatter
    clean_text = re.sub(r'^---\s+.*?\s+---\s+', '', full_text, flags=re.DOTALL).strip()
    
    if source_type == 'poc':
        pattern = re.compile(r'(?:^|\n)(?:#+\s*|\*\*)(æ¼æ´å¤ç°|POC|EXP|æ¼æ´POC)(?:\*\*|:)?.*?\n(.*?)(?:(?=\n#)|$)', re.IGNORECASE | re.DOTALL)
        match = pattern.search(clean_text)
        if match:
            content_body = match.group(2).strip()
    
    if not content_body:
        # å–é¦–ä¸ªéæ ‡é¢˜æ®µè½
        paragraphs = [p.strip() for p in clean_text.split('\n\n') if p.strip() and not p.strip().startswith('#')]
        if paragraphs:
            content_body = paragraphs[0]
            if len(content_body) < 100 and len(paragraphs) > 1:
                content_body += "\n\n" + paragraphs[1]

    # 4. å…³é”®è¯å—…æ¢ (æ’é™¤ Frontmatter)
    keywords_to_check = tag_rules.get('keywords', ["RCE", "å…æ€", "æƒé™ç»´æŒ", "å†…ç½‘æ¸—é€", "åº”æ€¥å“åº”", "æº¯æº"])
    for kw in keywords_to_check:
        if kw.lower() in clean_text.lower():
            if source_type == 'wiki' and kw == "é¢è¯•":
                tags.add("é¢è¯•ä¸æˆé•¿")
            else:
                tags.add(kw)

    # æ™ºèƒ½æˆªæ–­
    max_len = 1500
    if len(content_body) > max_len:
        content_body = content_body[:max_len] + "\n\n> ...... (æç¤º: å†…å®¹å·²æˆªæ–­ï¼Œè¯·æŸ¥çœ‹é™„ä»¶ `Markdown` è·å–å®Œæ•´ç»†èŠ‚)"
    
    # åŠ¨æ€å‰ç¼€
    prefix = "ğŸ’¡"
    tags_str = "".join(list(tags))
    if source_type == 'poc': prefix = "ğŸ›¡ï¸ [PoC]"
    elif "é¢è¯•" in tags_str: prefix = "ğŸ‘¨â€ğŸ’» [é¢è¯•]"
    elif "å·¥å…·" in tags_str: prefix = "ğŸ› ï¸ [å·¥å…·]"
    elif "çº¢è“å¯¹æŠ—" in tags_str or "çº¢é˜Ÿ" in tags_str: prefix = "âš”ï¸ [çº¢è“]"
    elif "ææƒ" in tags_str: prefix = "ğŸš€ [ææƒ]"
    elif "ä¿¡æ¯æ”¶é›†" in tags_str: prefix = "ğŸ” [ä¿¡æ¯æ”¶é›†]"
    
    formatted_content = f"{prefix}\n\n{content_body}"

    # é™„ä»¶å¤„ç†
    attachments = [file_path]
    dir_path = os.path.dirname(file_path)
    if os.path.exists(dir_path):
        for f in os.listdir(dir_path):
            if f.lower().endswith(('.pdf', '.docx', '.doc')) and os.path.join(dir_path, f) != file_path:
                attachments.append(os.path.join(dir_path, f))

    return {
        "title": final_title,
        "tags": list(tags),
        "content_path": file_path,
        "content_body": formatted_content,
        "attachments": attachments,
        "skip": skip
    }
